spring.config.activate.on-profile=dev,qa,stg,prod
server.port=8080

# Hibernate Properties
# The SQL dialect makes Hibernate generate better SQL for the chosen database
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect

# DB auth parameters
spring.datasource.url=${DB_JDBC_URL}
spring.datasource.username=${DB_USERNAME}
spring.datasource.password=${DB_PASSWORD}

# Liquibase
spring.liquibase.enabled=true
spring.liquibase.change-log=classpath:db/changelog/db.changelog-master.yaml
spring.liquibase.liquibase-schema=perso_analytics__db_changelog

# can be used to disable the Whitelabel Error Page
server.error.whitelabel.enabled=false

# Configuring health endpoint
management.endpoint.health.show-components=always
management.endpoints.web.base-path=/service/v1

# Configuring info endpoint
management.info.defaults.enabled=true
management.info.env.enabled=true
management.info.git.enabled=true
management.info.git.mode=simple
info.application.version=0.0.1
management.endpoints.web.exposure.include=info,health,batch-job

# Configs of spring-cloud-aws
cloud.aws.stack.auto=false
cloud.aws.region.auto=false
cloud.aws.region.static=${AWS_REGION}
spring.config.import=optional:aws-parameterstore:
aws.paramstore.prefix=/config
aws.paramstore.defaultContext=application
aws.paramstore.profileSeparator=_
aws.paramstore.name=${PARAM_STORE_NAME}

# Required connection configs for Kafka producer, consumer, and admin
spring.kafka.properties.sasl.mechanism=PLAIN
spring.kafka.properties.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVER}
spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='${KAFKA_SASL_USERNAME}' password='${KAFKA_SASL_PASSWORD}';
spring.kafka.properties.security.protocol=SASL_SSL
spring.kafka.properties.max.poll.interval.ms=900000
spring.kafka.properties.max.poll.records=250

# Required connection configs for Confluent Cloud Schema Registry
spring.kafka.properties.schema.registry.url=${SR_URL}
spring.kafka.properties.basic.auth.credentials.source=USER_INFO
spring.kafka.properties.basic.auth.user.info=${SR_API_KEY}:${SR_API_SECRET}

perks.eligible.opco.list=${PERKS_ELIGIBLE_OPCO_LIST}

# Best practice for higher availability in Apache Kafka clients prior to 3.0
spring.kafka.properties.session.timeout.ms=45000

# Spring batch properties
spring.batch.job.enabled=false
spring.batch.initialize-schema=ALWAYS
perks.fee.update.cron.expression=${BATCH_JOB_CRON_EXPRESSION}

#S3 upload config
dtc.offers.s3.bucket.name=${DTC_OFFERS_S3_BUCKET_NAME}
dtc.offers.s3.bucket.key=${DTC_OFFERS_S3_BUCKET_KEY}

#CRM API configuration
application.client.id=${APPLICATION_CLIENT_ID}
application.client.secret=${APPLICATION_CLIENT_SECRET}
oauth.token.url=${OAUTH_TOKEN_URL}
crm.api.v3.base.url=${CRM_API_BASE_URL}
salesforce.case.recordTypeId=${SALESFORCE_CASE_RECORD_TYPE_ID}
salesforce.case.ownerId=${SALESFORCE_CASE_OWNER_ID}
salesforce.lineItem.perksFee.supc=${SALESFORCE_PERKS_FEE_SUPC}
